<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://frontier-cs.org/feed.xml" rel="self" type="application/atom+xml"/><link href="https://frontier-cs.org/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-26T12:20:46-08:00</updated><id>https://frontier-cs.org/feed.xml</id><title type="html">Frontier-CS Blog Posts</title><subtitle>Home to Frontier-CS blog posts </subtitle><entry><title type="html">LLM Defeated in Open-ended Problems</title><link href="https://frontier-cs.org/blog/negative-optimization/" rel="alternate" type="text/html" title="LLM Defeated in Open-ended Problems"/><published>2026-02-26T00:00:00-08:00</published><updated>2026-02-26T00:00:00-08:00</updated><id>https://frontier-cs.org/blog/negative-optimization</id><content type="html" xml:base="https://frontier-cs.org/blog/negative-optimization/"><![CDATA[<p><img src="/assets/img/2026-02-14-negative-optimization/image2.png" alt="Performance Contrast: Algorithm vs. Heuristic" class="hero"/></p> <p>According to official technical reports, Deep Think’s model achieves a Codeforces elo rating of 3455. Our independent evaluation estimates Deep Think’s actual rating to be roughly 3299. However, during the open-ended AtCoder Heuristic Contest (AHC), the model only managed non-zero scores on under <strong>50%</strong> of the problems — a strong contrast to its performance on traditional algorithms. We conducted detailed cause analysis and case study based on the result.</p> <h2 id="the-real-world-evaluation-gap">The Real-World Evaluation Gap</h2> <p>In actual working environment, a consistent trend has been noticed widely: Modern LLMs’ (etc. Deep Think) effectiveness in actual work environments isn’t nearly as outstanding as their amazing performance on traditional algorithmic benchmarks shows.</p> <p>Unlike solving standard competitive programming problems with fixed boundaries, real-world work fundamentally tests a model’s capacity for continual learning and adaptation. There is a significant mismatch between current model behaviors and what traditional algorithmic benchmarks actually measure. To truly evaluate a model’s ability to handle complex, ambiguous scenarios, we must test it using open-ended problems.</p> <p>To illustrate this, we evaluated Deep Think on two entirely different tracks: 30 of the newest Codeforces problems with a high difficulty rating of 2500+ (representing traditional, strict-boundary algorithmic tasks), alongside all 33 short-format problems from the AtCoder Heuristic Contest (representing open-ended, heuristic challenges).</p> <p><img src="/assets/img/2026-02-14-negative-optimization/image1.png" alt="Performance Contrast: Algorithm vs. Heuristic" class="hero"/></p> <p>The data reveals a suprising contrast. We found that the model produces a massive volume of failure cases when attempting open-ended heuristic problems, given it’s near human. This high failure rate completely mismatches the model’s supposedly superhuman, high-confidence performance in traditional algorithmic domains.</p> <p>To understand exactly why this breakdown occurs and how models fall into these optimization traps, we conducted a detailed case study.</p> <h2 id="case-study-the-hedgehog-graph">Case Study: The Hedgehog Graph</h2> <p>To understand this collapse, let’s look at a specific interaction involving the interactive problem <strong>Hedgehog Graph</strong> (no. 222).</p> <ul> <li><strong>The Setup:</strong> You are given a directed graph with 1,000,000 vertices. Each vertex has exactly one outgoing edge, eventually flowing into a single directed cycle.</li> <li><strong>The Goal:</strong> Determine the length of the cycle, S.</li> <li><strong>The Tool:</strong> You can make queries <code class="language-plaintext highlighter-rouge">? v x</code>, returning the vertex reached by taking x steps from v.</li> <li><strong>The Constraint:</strong> You have a budget of 2500 queries.</li> <li><strong>The Score:</strong> Using 500 or fewer queries yields 100 points. The score decays quadratically up to 2500 queries, after which the score drops to 0.</li> </ul> <h3 id="round-1-the-robust-solution">Round 1: The Robust Solution</h3> <p>In its first attempt, the model defaulted to a highly stable, probabilistic approach based on the <strong>Birthday Paradox</strong>. It queried random step sizes, stored the resulting vertices, and looked for collisions.</p> \[E[\text{queries}] \approx \sqrt{\frac{\pi}{2} S}\] <p>For a cycle of 1,000,000 vertices, the expected number of queries is roughly 1200. This is a mathematically robust baseline. It doesn’t matter if S is prime or composite; the logic holds. The model correctly noted this would yield a partial score of around 20 points, well within the 2500 absolute limit.</p> <h3 id="round-2-incorrect-optimization">Round 2: Incorrect Optimization</h3> <p>A human competitor at this stage would secure the 20 points and attempt to safely optimize the constant factors or memory usage. The LLM, prompted to improve its score, took a wildly different path. It abandoned the stable baseline entirely for a “Scaled Collision” strategy.</p> <p>The model attempted to split its query budget into stages, testing highly composite numbers to exploit number theory and artificially force faster collisions.</p> <ul> <li><strong>Stage 1:</strong> Try factors of primes up to 47 for 220 queries.</li> <li><strong>Stage 2:</strong> Try factors of primes up to 43 for 220 queries.</li> <li><strong>Fallback:</strong> Use the remaining budget for random collisions.</li> </ul> <h2 id="the-anatomy-of-the-trap">The Anatomy of the Trap</h2> <p><img src="/assets/img/2026-02-14-negative-optimization/image5.jpg" alt="Performance Contrast: Algorithm vs. Heuristic" class="hero"/></p> <p>This looks like sophisticated engineering, but it is a catastrophic theoretical failure.</p> <p>The LLM optimized purely for the <strong>average case</strong> (composite numbers) while entirely blinding itself to the <strong>worst case</strong> (prime numbers). If the cycle length S is a large prime like 999,983, it shares no factors with the model’s composite guesses. The effective cycle length remains unchanged, and the “optimization” yields zero benefit.</p> <p>Then as a result, the model fragmented its strict query budget. By the time it realized its heuristic trick was failing on prime numbers, it had wasted 800 queries. Starting the fallback random search with a massive 800-query debt pushed the total past the 2500 limit.</p> <p><strong>The result: A stable 20-point baseline was “optimized” into a 0-point failure.</strong></p> <h2 id="the-post-training-flaw">The Post-Training Flaw</h2> <p>This behavior highlights a profound issue with modern LLMs. During post-training, AI institutions frequently optimize for state-of-the-art benchmark scores by artificially encouraging the model to take risks instead of stable choices. By generating multiple diverse rollouts and selecting only the highest-scoring one—a technique known as Best-of-N sampling—the training process disproportionately rewards high-variance, speculative logic over safe, incremental improvements. Consequently, the fully trained model naturally defaults to risky, “all-or-nothing” gambles rather than stable optimization methods.</p> <p><img src="/assets/img/2026-02-14-negative-optimization/image4.jpg" alt="Performance Contrast: Algorithm vs. Heuristic" class="hero"/></p> <p>When faced with a difficult optimization task lacking a strict, immediate verifier, the model resorts to a “lazy” strategy. Instead of doing the grueling work of designing a universally stable improvement, it speculates. It chooses unstable, point-cheating tricks that create the illusion of problem-solving but performs terribly under the pressure of strong, worst-case test data. The model effectively hit a cognitive ceiling, opting for giving an incorrect optimization rather than acknowledging the limits of its reasoning.</p> <h2 id="real-world-implications">Real-World Implications</h2> <p><img src="/assets/img/2026-02-14-negative-optimization/image6.jpg" alt="Performance Contrast: Algorithm vs. Heuristic" class="hero"/></p> <p>This is why continuous scoring in benchmarks like Frontier-CS is critical. In a binary pass/fail system, this regression might be masked.</p> <p>More importantly, the open-ended problems in Frontier-CS reveals the realities of actual software engineering. Production codebases and complex system architectures do not have a absolute, formal verifier. If we trust LLMs to autonomously optimize systems without strict bounds, they are highly intended to introducing fragile, edge-case-blind logic that works in theory but causes systemic collapse in practice.</p> <p>While LLMs undoubtedly possess elite capabilities for isolated, well-defined algorithms, their performance in open-ended reasoning remains fundamentally unreliable. Treating them as omnipotent architects rather than powerful, specialized tools is a trap we cannot afford to fall into.</p>]]></content><author><name>Wenyuan Huang</name></author><summary type="html"><![CDATA[Modern LLMs claim superhuman algorithmic abilities, but what happens when there is no strict verifier? We analyze how multi-turn 'optimization' in Frontier-CS exposes the cognitive ceiling and catastrophic failures of AI in open-ended problem solving.]]></summary></entry><entry><title type="html">Evaluating the Hardest CS Problems in the Age of LLMs</title><link href="https://frontier-cs.org/blog/evaluation/" rel="alternate" type="text/html" title="Evaluating the Hardest CS Problems in the Age of LLMs"/><published>2026-02-10T00:00:00-08:00</published><updated>2026-02-10T00:00:00-08:00</updated><id>https://frontier-cs.org/blog/evaluation</id><content type="html" xml:base="https://frontier-cs.org/blog/evaluation/"><![CDATA[<h2 id="what-is-frontiercs">What is FrontierCS?</h2> <p><a href="https://frontier-cs.org">Frontier-CS</a> is an open-source benchmark of 240 open-ended CS problems with continuous scoring.</p> <h2 id="evaluating-the-hardest-cs-problems-in-the-age-of-llms">Evaluating the Hardest CS Problems in the Age of LLMs</h2> <p><img src="/assets/img/2026-02-10-evaluation/image1.jpg" alt="Evaluation pipeline overview" class="hero"/></p> <p>Our <a href="https://frontier-cs.org/blog/feb-release/">1.0 release</a> introduced <strong>240 open-ended problems</strong> spanning algorithmic competition and systems research. The best model scores 46.55 on research tasks; human experts hit 86.99 on algorithmic ones. These numbers appear on the leaderboard, but how are they produced? And why should you trust them?</p> <p>This post is about what happens <em>after</em> a model writes its solution and <em>before</em> a score shows up on the leaderboard. For open-ended CS problems, <strong>evaluation is harder than generation</strong>.</p> <h2 id="what-is-so-hard-about-evaluation-anyway">What is so Hard about Evaluation Anyway?</h2> <p>Traditional coding benchmarks have a simple evaluation story: run the code, check the output, pass or fail. This is true whether you’re talking about LiveCodeBench checking competitive programming solutions, SWE-bench Verified checking if a GitHub issue got fixed, or Terminal-Bench checking if a DevOps task completed. The evaluation is binary, and it doesn’t depend on what hardware you run it on.</p> <p>Frontier-CS doesn’t work that way. Our problems are <strong>open-ended and continuous-scored</strong>. Optimize a CUDA kernel to be as fast as possible. Design a cache eviction policy that minimizes miss rate. Implement a parallel sort that scales across cores. There is no single correct answer, only a spectrum from “doesn’t compile” to “beats the reference implementation.”</p> <p>Evaluating a single problem means provisioning the right hardware, installing problem-specific dependencies, running the solution against a scoring harness, and collecting results. Now multiply that by 7 models, 240 problems, and 5 runs each. That’s 8,400 evaluations per cycle, and a new cycle starts every time a model provider ships an update.</p> <p><img src="/assets/img/2026-02-10-evaluation/image2.png" alt="Traditional vs Frontier-CS evaluation"/></p> <h2 id="240-problems-no-two-alike">240 Problems, No Two Alike</h2> <p>The 240 problems span two tracks with fundamentally different evaluation needs:</p> <ul> <li><strong>Algorithmic track</strong> (172 problems): C++ solutions judged by a <a href="https://github.com/criyle/go-judge">go-judge</a> server against test cases with custom scoring functions. Runs on CPU.</li> <li><strong>Research track</strong> (68 problems, with 127 evaluators total, since some problems have multiple evaluation criteria): Python solutions that might need GPUs, specific CUDA versions, or custom Python packages installed via <code class="language-plaintext highlighter-rouge">uv</code>. A single problem can take 15 minutes to evaluate.</li> </ul> <p>A CUDA kernel optimization problem needs an A100. An algorithm problem needs nothing more than a CPU and a C++ compiler. You can’t run these on the same machine with the same setup. Provisioning one beefy GPU cluster for everything wastes money. Cramming everything onto one machine wastes time, or simply fails.</p> <p>We needed an evaluation system that handles this heterogeneity automatically, runs continuously, and produces scores you can trust across months of leaderboard updates. Here’s what we built.</p> <h2 id="the-architecture-in-brief">The Architecture in Brief</h2> <p>The system has two layers. <strong><code class="language-plaintext highlighter-rouge">SingleEvaluator</code></strong> handles one-off runs: a contributor iterating locally, wanting a quick score. <strong><code class="language-plaintext highlighter-rouge">BatchEvaluator</code></strong> handles scale: the weekly CI job that evaluates every model on every problem. Both share the same runners underneath, so a score produced locally via Docker is comparable to one produced in the cloud via SkyPilot.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Runner
├── ResearchRunner          (shared validation + config)
│   ├── ResearchDockerRunner
│   └── ResearchSkyPilotRunner
├── AlgorithmicLocalRunner
└── AlgorithmicSkyPilotRunner
</code></pre></div></div> <p>The interesting part isn’t the class hierarchy. It’s the set of problems we had to solve to make this work reliably at scale, and why those problems are specific to evaluating LLMs on open-ended tasks.</p> <h2 id="four-challenges-of-the-llm-era">Four Challenges of the LLM Era</h2> <h3 id="1-continuous-evaluation-not-one-shot">1. Continuous evaluation, not one-shot</h3> <p>Academic benchmarks are typically evaluated once for a paper. Frontier-CS is a <strong>living leaderboard</strong>. Just in the past few months: Gemini 3.0 Pro, GPT 5.2 Thinking, Grok 4, DeepSeek 3.2. And the pace is accelerating, not slowing down. As we write this, OpenAI has just released GPT-5.3-codex and Anthropic has shipped Claude Opus 4.6. Each new model needs to be evaluated across all 240 problems. That’s thousands of evaluation runs per cycle, and the cycles keep getting shorter.</p> <p>This forced us to build <strong>hash-based resume</strong>. Every result records the SHA hash of both the solution file and the problem directory. When a batch is resumed, changed inputs are automatically re-evaluated while unchanged results are kept. Without this, we’d either waste compute re-running everything, or risk publishing stale scores after a problem’s scoring harness gets updated.</p> <h3 id="2-heterogeneous-hardware-at-scale">2. Heterogeneous hardware at scale</h3> <p>A single batch run might include problems that need an A100 GPU, problems that need a specific AWS instance for HPC workloads (our n-body simulation problems require a <code class="language-plaintext highlighter-rouge">c7i.4xlarge</code> with 16 vCPUs for OpenMP parallelism), and problems that need nothing special.</p> <p>We solve this with <strong>resource-grouped cluster pools</strong>. Before evaluation starts, pairs are grouped by their <code class="language-plaintext highlighter-rouge">ResourceSignature</code>, a tuple of (cloud, accelerators, instance type) derived from each problem’s <code class="language-plaintext highlighter-rouge">config.yaml</code>. Each group gets its own pool of SkyPilot clusters, sized proportionally to the number of problems in that group.</p> <p><img src="/assets/img/2026-02-10-evaluation/image3.png" alt="Resource-grouped cluster pools"/></p> <p>CPU-only problems run on cheap instances. GPU problems get GPU instances. No cluster sits idle running the wrong workload.</p> <h3 id="3-evaluation-determinism">3. Evaluation determinism</h3> <p>This is the one that keeps benchmark maintainers up at night.</p> <p>SWE-bench Verified checks whether a patch makes tests pass. LiveCodeBench checks whether a solution produces the right output. Those scores are the same whether you run them on a laptop or a cloud VM. Our problems are <strong>latency-sensitive</strong>. The score <em>is</em> how fast your code runs, how much throughput it achieves, how efficiently it uses the hardware. Swap an A100 for an H100, and every number on the leaderboard changes.</p> <p>This makes our evaluation environments far more complex than even the most sophisticated agentic benchmarks. SWE-bench Verified’s environment is a git repo and a test suite. A Frontier-CS research problem might require a specific GPU, a pinned CUDA version, custom Python packages, and a large dataset pre-loaded into memory. Setting up a single problem’s environment can be more involved than SWE-bench’s entire evaluation pipeline.</p> <p>The RL community learned how fragile this can be when MuJoCo and Gym version upgrades silently invalidated years of published baselines.</p> <p>Our answer is to <strong>pin everything and invalidate aggressively</strong>. Each problem’s <code class="language-plaintext highlighter-rouge">config.yaml</code> locks down the hardware spec, CUDA version, and runtime environment. The execution environment itself is pinned via Docker images and AMIs, so every run sees the exact same OS, drivers, and libraries. We also encourage problem contributors to design evaluators with deterministic scoring: given the same solution and the same environment, the score should be reproducible. Every evaluation result is then tagged with hashes of both the solution and the entire problem directory (including the evaluator code and config). If <em>anything</em> changes, the hash changes, and all affected scores are automatically invalidated and re-run.</p> <p>We don’t try to make scores portable across environments. Instead, we guarantee that <strong>every valid score on the leaderboard was produced under the same pinned conditions</strong>. Determinism through immutability, not through cross-environment normalization.</p> <p><img src="/assets/img/2026-02-10-evaluation/image4.png" alt="Pin, hash, and invalidate cycle"/></p> <p>Evaluation determinism is becoming a first-class concern across the benchmarking community. Projects like <a href="https://github.com/cocoabench/cocoa-agent">CocoaBench</a> are pushing this idea further, building frameworks specifically around reproducible evaluation environments. We think this is the right direction.</p> <h3 id="4-generation--evaluation">4. Generation ≠ Evaluation</h3> <p>Many LLM benchmarks tightly couple generation and evaluation: call the API, get the output, score it immediately. We deliberately <strong>decouple</strong> them.</p> <p>The <code class="language-plaintext highlighter-rouge">gen/</code> module calls LLMs and writes plain source files (<code class="language-plaintext highlighter-rouge">solution.py</code> or <code class="language-plaintext highlighter-rouge">solution.cpp</code>). The evaluation pipeline doesn’t know or care which model wrote the code, what prompt was used, or how many attempts it took. Each evaluation runs in an <strong>isolated container or VM</strong>: solutions cannot see each other, cannot read test cases, and cannot access the network.</p> <p>Why does this matter? A human expert can drop a hand-written solution into the same directory and it gets evaluated identically. You can compare different prompting strategies, different temperatures, or different models on exactly the same evaluation path. And because each run is sandboxed, there’s no way for a solution to game the evaluator by reading other solutions or probing the scoring harness. The scoring is <strong>model-agnostic and tamper-resistant by design</strong>.</p> <h2 id="looking-ahead-agentic-submissions">Looking Ahead: Agentic Submissions</h2> <p>The clean separation above also forces us to confront a question the community is only starting to grapple with: <strong>what happens when the submission is an agent, not a file?</strong></p> <p>Today, a submission to Frontier-CS is a source file. But the frontier is moving toward agentic workflows where the model reads the problem, writes code, runs it, inspects the output, and iterates. The generation process itself has side effects: it reads the environment, runs tests, modifies code across multiple turns. The boundary between “generating a solution” and “evaluating it” starts to blur.</p> <p><img src="/assets/img/2026-02-10-evaluation/image5.png" alt="File-based vs agentic submission pipeline"/></p> <p>Our current architecture handles this by keeping the boundary sharp: no matter how the solution was produced (single-shot, chain-of-thought, 50 rounds of agentic iteration), what enters the evaluation pipeline is still a source file. The agent’s process is its own business; we only score the artifact.</p> <p>But as agents get more capable, two challenges loom.</p> <p>First, <strong>reward hacking</strong>. An agent that iterates against a scoring harness is doing what any good engineer does: profile, optimize, repeat. The risk is that as agents get stronger, they overfit harder. And not all overfitting is equal. An agent that discovers temporal locality in a cache trace and designs a policy around it has learned something real. An agent that memorizes the specific access pattern in the test set and hardcodes a lookup table has learned nothing. Both produce high scores. Only one generalizes. The stronger the agent, the greater the risk of spurious overfitting, and the more pressure on us to design evaluation data that faithfully represents the real problem, so that gaming the harness and genuinely solving it become indistinguishable.</p> <p>Second, <strong>the cost model breaks</strong>. Right now, generation (API calls) and evaluation (GPU compute) are cleanly separated. You call the API, get a file, and schedule GPU time whenever it’s convenient. With agentic submissions, the agent needs to write code, run it on a GPU, inspect the results, and iterate. The GPU cluster has to stay alive while the agent thinks and makes API calls. You’re burning two expensive resources simultaneously, and each one is idle while waiting for the other. This is fundamentally a co-scheduling problem between API latency and GPU utilization, and it doesn’t have an obvious solution yet.</p> <h2 id="what-a-batch-run-looks-like">What a Batch Run Looks Like</h2> <p>Here’s what happens when the weekly CI job runs:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontier batch research <span class="nt">--workers</span> 10 <span class="nt">--clusters</span> 10 <span class="nt">--backend</span> skypilot
</code></pre></div></div> <ol> <li><strong>Discover pairs</strong>: scan the solutions directory, match each solution file to its problem.</li> <li><strong>Check hashes</strong>: compare solution/problem hashes against stored results. Skip unchanged pairs; invalidate stale results.</li> <li><strong>Group by resources</strong>: partition pairs by <code class="language-plaintext highlighter-rouge">ResourceSignature</code>. A CUDA kernel problem and a CPU algorithm problem land in different groups.</li> <li><strong>Create cluster pools</strong>: provision SkyPilot clusters for each group, proportionally allocated.</li> <li><strong>Dispatch</strong>: workers pull pairs from a shared queue, acquire a cluster from the matching pool, run the evaluation, return the cluster.</li> <li><strong>Record</strong>: each result is saved with its hashes. Interrupted runs resume from exactly where they left off.</li> </ol> <p>The same pipeline runs for the algorithmic track, with a go-judge server instead of SkyPilot clusters.</p> <h2 id="what-this-enables">What This Enables</h2> <ul> <li><strong>The leaderboard stays honest.</strong> Every score is tagged with environment hashes. Stale results are caught automatically. Failures are tracked via status metadata, not silently recorded as score = 0.</li> <li><strong>Contributors can validate locally.</strong> <code class="language-plaintext highlighter-rouge">frontier eval</code> runs the same scoring logic on your laptop (via Docker) that the CI runs in the cloud.</li> <li><strong>New models get evaluated fast.</strong> Hash-based resume means only new or changed pairs are evaluated. Right-sized cluster pools mean no wasted compute.</li> </ul> <p><img src="/assets/img/2026-02-10-evaluation/image6.png" alt="Batch run terminal output (mockup)"/></p> <h2 id="try-it-yourself">Try It Yourself</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/FrontierCS/Frontier-CS.git
<span class="nb">cd </span>Frontier-CS <span class="o">&amp;&amp;</span> uv <span class="nb">sync</span>

<span class="c"># Evaluate a single problem locally</span>
frontier <span class="nb">eval </span>research flash_attn solution.py <span class="nt">--backend</span> docker

<span class="c"># List all available problems</span>
frontier list
</code></pre></div></div> <p>For the full architecture details, see <a href="https://github.com/FrontierCS/Frontier-CS/blob/main/ARCHITECTURE.md">ARCHITECTURE.md</a> in the repo.</p> <h2 id="getting-in-touch">Getting in Touch</h2> <p>We are always looking for more problems to add for our next release and evaluating on <a href="https://github.com/FrontierCS/Frontier-CS/blob/main/SUBMIT.md">more models and agents</a>. We love to hear about your comments and feedback! Join us on <a href="https://discord.com/invite/k4hd2nU4UE">discord</a> or <a href="mailto:frontiercs@berkeley.edu">email us</a>!</p> <p>And if you find Frontier-CS useful, please consider <a href="https://github.com/FrontierCS/Frontier-CS">giving us a star on GitHub</a>!</p>]]></content><author><name>Zhifei Li</name></author><summary type="html"><![CDATA[Frontier-CS scores solutions on a continuous scale across heterogeneous hardware. This post explains the evaluation architecture behind the leaderboard: hash-based resume, resource-grouped clusters, pinned environments, and the challenges ahead for agentic submissions.]]></summary></entry><entry><title type="html">Frontier-CS 1.0 Release</title><link href="https://frontier-cs.org/blog/feb-release/" rel="alternate" type="text/html" title="Frontier-CS 1.0 Release"/><published>2026-02-03T00:00:00-08:00</published><updated>2026-02-03T00:00:00-08:00</updated><id>https://frontier-cs.org/blog/feb-release</id><content type="html" xml:base="https://frontier-cs.org/blog/feb-release/"><![CDATA[<p><img src="https://raw.githubusercontent.com/FrontierCS/Frontier-CS/main/assets/logo.png" alt=""/></p> <p>We’re excited to announce the release of <strong><a href="https://github.com/FrontierCS/Frontier-CS">Frontier-CS 1.0</a></strong>! Frontier-CS is an open-ended benchmark designed for evolving intelligence. It now comprises <strong>240 unsolved problems (+94 since Dec 2025)</strong>, spanning a diverse range of computer-science domains and are authored by CS PhDs and ICPC World Final–level experts.</p> <p>Frontier-CS supports benchmarking frontier models, agent-based evaluation, algorithm evolution, post-training, and beyond - any setting where progress is measured by <strong>quantitative, fine-grained metrics that reflect solution quality</strong>, rather than binary pass/fail outcomes.</p> <h2 id="why-frontier-cs">Why Frontier-CS?</h2> <p>By 2025, LLMs have largely saturated exam-style benchmarks in computer science. We now see near-perfect performance on standardized evaluations: ~80% on SWE-bench Verified, gold-medal–level results at the 2025 ICPC World Finals, and strong scores across many exam-style tasks. While useful, these benchmarks no longer capture meaningful progress in the development of foundational models and various modes of problem solving.</p> <p>In light of that, Frontier-CS is motivated by two gaps. First, we need open-ended tasks that never saturate, so that progress can be continuously made and reflects long-horizon planning, code writing, and genuine skills in problem-solving—not binary pass/fail accuracy hacking. Second, with the rise of evolution and agentic frameworks (e.g., AlphaEvolve, ADRS, TTT-style discovery), we need a large-scale, verifiable benchmark that enables comprehensive comparison and provides training signals toward scalable agents for scientific discovery, including agentic RL.</p> <h2 id="so-how-do-the-models-do">So how do the models do?</h2> <p>Crafted by several ICPC World Finalists and CS PhDs, Frontier-CS has 240 problems which are <strong>open-ended</strong>, <strong>verifiable</strong>, and <strong>diverse</strong> (varying 7 research domains and 3 algorithmic engineering categories). Surprisingly, even though modern models have nearly aced traditional computer science benchmarks, they still struggle with the open-ended nature of Frontier-CS. On the algorithmic track, human expert solutions achieve an average score of <strong>86.99</strong>, while the strongest model reaches only <strong>33.12</strong>. This substantial gap remains far from closed. We’ll go into the details of our results in a later post, but for now check out our <a href="https://github.com/FrontierCS/Frontier-CS?tab=readme-ov-file#-leaderboard-snapshot-01292026">leaderboard</a>.</p> <p><img src="https://raw.githubusercontent.com/FrontierCS/Frontier-CS/main/assets/teaser.png" alt="Teaser Image"/></p> <h2 id="whats-new-in-frontier-cs-10">What’s New in Frontier-CS 1.0</h2> <ul> <li> <p><strong>Expanded Problem Set</strong>: Frontier-CS now includes 240 problems, an increase of 94 since December 2025. This expansion spans a broader range of computer science topics and challenges. For example, we introduce two new software-engineering fuzzing tasks, where models must design fuzzers that maximize code coverage under a fixed budget. We also added more algorithmic problems, including several classical NP-hard challenges and tasks adapted from the latest competitive programming contests.</p> </li> <li> <p><strong>Enhanced Evaluation Metrics</strong>: In this update, we introduce Elo ratings to Frontier-CS. This addition is motivated by a simple issue with raw scores in open-ended benchmarks: they are often hard to interpret across problems with vastly different difficulty and scoring ranges. For example, it is unclear how to compare a model that outperforms another by 10 points on one problem with one that wins by only 1 point on a different problem. Elo ratings address this by focusing on relative, pairwise performance rather than absolute score magnitudes, resulting in a leaderboard that better reflects overall model capability and remains stable as new problems are added.</p> </li> </ul> <p><img src="https://raw.githubusercontent.com/FrontierCS/Frontier-CS/main/assets/result-table.png" alt="Teaser Image"/></p> <ul> <li> <p><strong>Releasing model trajectories</strong>: We release over <a href="https://github.com/FrontierCS/Frontier-CS/tree/main/algorithmic/solutions">3,000 solution trajectories</a> from seven frontier models evaluated on Frontier-CS, including GPT-5.2, GPT-5.1, GPT-5, Gemini 3.0 Pro, Gemini 2.5 Pro, Grok-4, and DeepSeek-3.2. These trajectories enable fine-grained analysis of model behavior and provide a valuable resource for future research on model understanding and performance improvement.</p> </li> <li> <p><strong>Enhanced user experience</strong>: We’ve refined the Frontier-CS workflow to make evaluation simpler and more intuitive. Solutions can now be evaluated with a single command or through a clean, easy-to-use API, with complex, problem-specific environments unified into a consistent execution interface without setup by users.</p> </li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/FrontierCS/Frontier-CS.git
<span class="nb">cd </span>Frontier-CS
<span class="c"># Install dependencies (using uv, recommended)</span>
uv <span class="nb">sync</span>
<span class="c"># Run the example solution (GPT-5 Thinking Solution)</span>
frontier <span class="nb">eval</span> <span class="nt">--algorithmic</span> 0 algorithmic/problems/0/examples/gpt5.cpp
</code></pre></div></div> <ul> <li><strong>Agentic Frameworks</strong>: We are exploring the integration of agentic frameworks into Frontier-CS (e.g., OpenEvolve), and we welcome submissions of agent results to our leaderboard.</li> </ul> <h2 id="getting-in-touch">Getting in Touch</h2> <p>We are always looking for more problems to add for our next release and evaluating on <a href="https://github.com/FrontierCS/Frontier-CS/blob/main/SUBMIT.md">more models and agents</a>. We love to hear about your comments and feedback! Join us on <a href="https://discord.com/invite/k4hd2nU4UE">discord</a> or <a href="mailto:frontiercs@berkeley.edu">email us</a>!</p> <p>And if you find Frontier-CS useful, please consider <a href="https://github.com/FrontierCS/Frontier-CS">giving us a ⭐ on GitHub</a>!</p>]]></content><author><name>Alvin Cheung</name></author><summary type="html"><![CDATA[We are releasing Frontier-CS 1.0, a major update to our open-ended Computer Science benchmark. This release expands Frontier-CS to 240 tasks across both the algorithmic and research tracks. We also introduce a new Elo-based leaderboard, along with full execution traces of model solutions to enable deeper analysis and reproducibility.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://frontier-cs.org/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2025-04-28T00:00:00-07:00</published><updated>2025-04-28T00:00:00-07:00</updated><id>https://frontier-cs.org/blog/distill-example</id><content type="html" xml:base="https://frontier-cs.org/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1772137253225" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1772137253225 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1772137253225 .node circle,#mermaid-1772137253225 .node ellipse,#mermaid-1772137253225 .node polygon,#mermaid-1772137253225 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1772137253225 .node.clickable{cursor:pointer}#mermaid-1772137253225 .arrowheadPath{fill:#333}#mermaid-1772137253225 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1772137253225 .edgeLabel{background-color:#e8e8e8}#mermaid-1772137253225 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1772137253225 .cluster text{fill:#333}#mermaid-1772137253225 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1772137253225 .actor{stroke:#ccf;fill:#ececff}#mermaid-1772137253225 text.actor{fill:#000;stroke:none}#mermaid-1772137253225 .actor-line{stroke:grey}#mermaid-1772137253225 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1772137253225 .messageLine0,#mermaid-1772137253225 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1772137253225 #arrowhead{fill:#333}#mermaid-1772137253225 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1772137253225 .messageText{fill:#333;stroke:none}#mermaid-1772137253225 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1772137253225 .labelText,#mermaid-1772137253225 .loopText{fill:#000;stroke:none}#mermaid-1772137253225 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1772137253225 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1772137253225 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1772137253225 .section{stroke:none;opacity:.2}#mermaid-1772137253225 .section0{fill:rgba(102,102,255,.49)}#mermaid-1772137253225 .section2{fill:#fff400}#mermaid-1772137253225 .section1,#mermaid-1772137253225 .section3{fill:#fff;opacity:.2}#mermaid-1772137253225 .sectionTitle0,#mermaid-1772137253225 .sectionTitle1,#mermaid-1772137253225 .sectionTitle2,#mermaid-1772137253225 .sectionTitle3{fill:#333}#mermaid-1772137253225 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1772137253225 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1772137253225 .grid path{stroke-width:0}#mermaid-1772137253225 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1772137253225 .task{stroke-width:2}#mermaid-1772137253225 .taskText{text-anchor:middle;font-size:11px}#mermaid-1772137253225 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1772137253225 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1772137253225 .taskText0,#mermaid-1772137253225 .taskText1,#mermaid-1772137253225 .taskText2,#mermaid-1772137253225 .taskText3{fill:#fff}#mermaid-1772137253225 .task0,#mermaid-1772137253225 .task1,#mermaid-1772137253225 .task2,#mermaid-1772137253225 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1772137253225 .taskTextOutside0,#mermaid-1772137253225 .taskTextOutside1,#mermaid-1772137253225 .taskTextOutside2,#mermaid-1772137253225 .taskTextOutside3{fill:#000}#mermaid-1772137253225 .active0,#mermaid-1772137253225 .active1,#mermaid-1772137253225 .active2,#mermaid-1772137253225 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1772137253225 .activeText0,#mermaid-1772137253225 .activeText1,#mermaid-1772137253225 .activeText2,#mermaid-1772137253225 .activeText3{fill:#000!important}#mermaid-1772137253225 .done0,#mermaid-1772137253225 .done1,#mermaid-1772137253225 .done2,#mermaid-1772137253225 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1772137253225 .doneText0,#mermaid-1772137253225 .doneText1,#mermaid-1772137253225 .doneText2,#mermaid-1772137253225 .doneText3{fill:#000!important}#mermaid-1772137253225 .crit0,#mermaid-1772137253225 .crit1,#mermaid-1772137253225 .crit2,#mermaid-1772137253225 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1772137253225 .activeCrit0,#mermaid-1772137253225 .activeCrit1,#mermaid-1772137253225 .activeCrit2,#mermaid-1772137253225 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1772137253225 .doneCrit0,#mermaid-1772137253225 .doneCrit1,#mermaid-1772137253225 .doneCrit2,#mermaid-1772137253225 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1772137253225 .activeCritText0,#mermaid-1772137253225 .activeCritText1,#mermaid-1772137253225 .activeCritText2,#mermaid-1772137253225 .activeCritText3,#mermaid-1772137253225 .doneCritText0,#mermaid-1772137253225 .doneCritText1,#mermaid-1772137253225 .doneCritText2,#mermaid-1772137253225 .doneCritText3{fill:#000!important}#mermaid-1772137253225 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1772137253225 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1772137253225 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1772137253225 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1772137253225 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1772137253225 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1772137253225 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1772137253225 #compositionEnd,#mermaid-1772137253225 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1772137253225 #aggregationEnd,#mermaid-1772137253225 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1772137253225 #dependencyEnd,#mermaid-1772137253225 #dependencyStart,#mermaid-1772137253225 #extensionEnd,#mermaid-1772137253225 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1772137253225 .branch-label,#mermaid-1772137253225 .commit-id,#mermaid-1772137253225 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1772137253225{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item <ul> <li>Unordered sub-list.</li> </ul> </li> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>