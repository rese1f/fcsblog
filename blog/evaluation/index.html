<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Evaluating the Hardest CS Problems in the Age of LLMs | Frontier-CS Blog Posts</title> <meta name="author" content="Frontier-CS Blog"> <meta name="description" content="Frontier-CS scores solutions on a continuous scale across heterogeneous hardware. This post explains the evaluation architecture behind the leaderboard: hash-based resume, resource-grouped clusters, pinned environments, and the challenges ahead for agentic submissions."> <meta name="keywords" content="machine-learning, ml, deep-learning, frontier-cs, blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2032%2032'%3E%3Crect%20width='32'%20height='32'%20fill='%23003262'%20rx='4'/%3E%3Ctext%20x='16'%20y='22'%20font-family='Georgia,%20serif'%20font-size='20'%20font-weight='bold'%20fill='white'%20text-anchor='middle'%3EB%3C/text%3E%3C/svg%3E"> <link rel="shortcut icon" href="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2032%2032'%3E%3Crect%20width='32'%20height='32'%20fill='%23003262'%20rx='4'/%3E%3Ctext%20x='16'%20y='22'%20font-family='Georgia,%20serif'%20font-size='20'%20font-weight='bold'%20fill='white'%20text-anchor='middle'%3EB%3C/text%3E%3C/svg%3E"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://frontier-cs.org/blog/evaluation/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">d-article img{max-width:70%;height:auto;display:block;margin:1.5rem auto;border-radius:6px}d-article img.hero{max-width:100%}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Evaluating the Hardest CS Problems in the Age of LLMs",
      "description": "Frontier-CS scores solutions on a continuous scale across heterogeneous hardware. This post explains the evaluation architecture behind the leaderboard: hash-based resume, resource-grouped clusters, pinned environments, and the challenges ahead for agentic submissions.",
      "published": "2026-02-10T12:00:00Z",
      "authors": [
        {
          "author": "Zhifei Li",
          "authorURL": "https://andylizf.github.io/",
          "affiliations": [
            {
              "name": "UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Hanchen Li",
          "authorURL": "https://hanchenli.github.io/",
          "affiliations": [
            {
              "name": "UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Qiuyang Mang",
          "authorURL": "https://joyemang33.github.io",
          "affiliations": [
            {
              "name": "UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Wenhao Chai",
          "authorURL": "https://wenhaochai.com/",
          "affiliations": [
            {
              "name": "Princeton University",
              "url": ""
            }
          ]
        },
        {
          "author": "Frontier-CS team",
          "authorURL": "https://frontier-cs.org",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Frontier-CS blog posts </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Evaluating the Hardest CS Problems in the Age of LLMs</h1> <p>Frontier-CS scores solutions on a continuous scale across heterogeneous hardware. This post explains the evaluation architecture behind the leaderboard: hash-based resume, resource-grouped clusters, pinned environments, and the challenges ahead for agentic submissions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-is-frontiercs">What is FrontierCS?</a></div> <div><a href="#what-is-so-hard-about-evaluation-anyway">What is so Hard about Evaluation Anyway?</a></div> <div><a href="#240-problems-no-two-alike">240 Problems, No Two Alike</a></div> <div><a href="#the-architecture-in-brief">The Architecture in Brief</a></div> <div><a href="#four-challenges-of-the-llm-era">Four Challenges of the LLM Era</a></div> <div><a href="#looking-ahead-agentic-submissions">Looking Ahead - Agentic Submissions</a></div> <div><a href="#what-a-batch-run-looks-like">What a Batch Run Looks Like</a></div> <div><a href="#what-this-enables">What This Enables</a></div> <div><a href="#try-it-yourself">Try It Yourself</a></div> <div><a href="#getting-in-touch">Getting in Touch</a></div> </nav> </d-contents> <h2 id="what-is-frontiercs">What is FrontierCS?</h2> <p><a href="https://frontier-cs.org">Frontier-CS</a> is an open-source benchmark of 240 open-ended CS problems with continuous scoring.</p> <h2 id="evaluating-the-hardest-cs-problems-in-the-age-of-llms">Evaluating the Hardest CS Problems in the Age of LLMs</h2> <p><img src="/assets/img/2026-02-10-evaluation/image1.jpg" alt="Evaluation pipeline overview" class="hero"></p> <p>Our <a href="https://frontier-cs.org/blog/feb-release/">1.0 release</a> introduced <strong>240 open-ended problems</strong> spanning algorithmic competition and systems research. The best model scores 46.55 on research tasks; human experts hit 86.99 on algorithmic ones. These numbers appear on the leaderboard, but how are they produced? And why should you trust them?</p> <p>This post is about what happens <em>after</em> a model writes its solution and <em>before</em> a score shows up on the leaderboard. For open-ended CS problems, <strong>evaluation is harder than generation</strong>.</p> <h2 id="what-is-so-hard-about-evaluation-anyway">What is so Hard about Evaluation Anyway?</h2> <p>Traditional coding benchmarks have a simple evaluation story: run the code, check the output, pass or fail. This is true whether you’re talking about LiveCodeBench checking competitive programming solutions, SWE-bench Verified checking if a GitHub issue got fixed, or Terminal-Bench checking if a DevOps task completed. The evaluation is binary, and it doesn’t depend on what hardware you run it on.</p> <p>Frontier-CS doesn’t work that way. Our problems are <strong>open-ended and continuous-scored</strong>. Optimize a CUDA kernel to be as fast as possible. Design a cache eviction policy that minimizes miss rate. Implement a parallel sort that scales across cores. There is no single correct answer, only a spectrum from “doesn’t compile” to “beats the reference implementation.”</p> <p>Evaluating a single problem means provisioning the right hardware, installing problem-specific dependencies, running the solution against a scoring harness, and collecting results. Now multiply that by 7 models, 240 problems, and 5 runs each. That’s 8,400 evaluations per cycle, and a new cycle starts every time a model provider ships an update.</p> <p><img src="/assets/img/2026-02-10-evaluation/image2.png" alt="Traditional vs Frontier-CS evaluation"></p> <h2 id="240-problems-no-two-alike">240 Problems, No Two Alike</h2> <p>The 240 problems span two tracks with fundamentally different evaluation needs:</p> <ul> <li> <strong>Algorithmic track</strong> (172 problems): C++ solutions judged by a <a href="https://github.com/criyle/go-judge" rel="external nofollow noopener noopener noreferrer" target="_blank">go-judge</a> server against test cases with custom scoring functions. Runs on CPU.</li> <li> <strong>Research track</strong> (68 problems, with 127 evaluators total, since some problems have multiple evaluation criteria): Python solutions that might need GPUs, specific CUDA versions, or custom Python packages installed via <code class="language-plaintext highlighter-rouge">uv</code>. A single problem can take 15 minutes to evaluate.</li> </ul> <p>A CUDA kernel optimization problem needs an A100. An algorithm problem needs nothing more than a CPU and a C++ compiler. You can’t run these on the same machine with the same setup. Provisioning one beefy GPU cluster for everything wastes money. Cramming everything onto one machine wastes time, or simply fails.</p> <p>We needed an evaluation system that handles this heterogeneity automatically, runs continuously, and produces scores you can trust across months of leaderboard updates. Here’s what we built.</p> <h2 id="the-architecture-in-brief">The Architecture in Brief</h2> <p>The system has two layers. <strong><code class="language-plaintext highlighter-rouge">SingleEvaluator</code></strong> handles one-off runs: a contributor iterating locally, wanting a quick score. <strong><code class="language-plaintext highlighter-rouge">BatchEvaluator</code></strong> handles scale: the weekly CI job that evaluates every model on every problem. Both share the same runners underneath, so a score produced locally via Docker is comparable to one produced in the cloud via SkyPilot.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Runner
├── ResearchRunner          (shared validation + config)
│   ├── ResearchDockerRunner
│   └── ResearchSkyPilotRunner
├── AlgorithmicLocalRunner
└── AlgorithmicSkyPilotRunner
</code></pre></div></div> <p>The interesting part isn’t the class hierarchy. It’s the set of problems we had to solve to make this work reliably at scale, and why those problems are specific to evaluating LLMs on open-ended tasks.</p> <h2 id="four-challenges-of-the-llm-era">Four Challenges of the LLM Era</h2> <h3 id="1-continuous-evaluation-not-one-shot">1. Continuous evaluation, not one-shot</h3> <p>Academic benchmarks are typically evaluated once for a paper. Frontier-CS is a <strong>living leaderboard</strong>. Just in the past few months: Gemini 3.0 Pro, GPT 5.2 Thinking, Grok 4, DeepSeek 3.2. And the pace is accelerating, not slowing down. As we write this, OpenAI has just released GPT-5.3-codex and Anthropic has shipped Claude Opus 4.6. Each new model needs to be evaluated across all 240 problems. That’s thousands of evaluation runs per cycle, and the cycles keep getting shorter.</p> <p>This forced us to build <strong>hash-based resume</strong>. Every result records the SHA hash of both the solution file and the problem directory. When a batch is resumed, changed inputs are automatically re-evaluated while unchanged results are kept. Without this, we’d either waste compute re-running everything, or risk publishing stale scores after a problem’s scoring harness gets updated.</p> <h3 id="2-heterogeneous-hardware-at-scale">2. Heterogeneous hardware at scale</h3> <p>A single batch run might include problems that need an A100 GPU, problems that need a specific AWS instance for HPC workloads (our n-body simulation problems require a <code class="language-plaintext highlighter-rouge">c7i.4xlarge</code> with 16 vCPUs for OpenMP parallelism), and problems that need nothing special.</p> <p>We solve this with <strong>resource-grouped cluster pools</strong>. Before evaluation starts, pairs are grouped by their <code class="language-plaintext highlighter-rouge">ResourceSignature</code>, a tuple of (cloud, accelerators, instance type) derived from each problem’s <code class="language-plaintext highlighter-rouge">config.yaml</code>. Each group gets its own pool of SkyPilot clusters, sized proportionally to the number of problems in that group.</p> <p><img src="/assets/img/2026-02-10-evaluation/image3.png" alt="Resource-grouped cluster pools"></p> <p>CPU-only problems run on cheap instances. GPU problems get GPU instances. No cluster sits idle running the wrong workload.</p> <h3 id="3-evaluation-determinism">3. Evaluation determinism</h3> <p>This is the one that keeps benchmark maintainers up at night.</p> <p>SWE-bench Verified checks whether a patch makes tests pass. LiveCodeBench checks whether a solution produces the right output. Those scores are the same whether you run them on a laptop or a cloud VM. Our problems are <strong>latency-sensitive</strong>. The score <em>is</em> how fast your code runs, how much throughput it achieves, how efficiently it uses the hardware. Swap an A100 for an H100, and every number on the leaderboard changes.</p> <p>This makes our evaluation environments far more complex than even the most sophisticated agentic benchmarks. SWE-bench Verified’s environment is a git repo and a test suite. A Frontier-CS research problem might require a specific GPU, a pinned CUDA version, custom Python packages, and a large dataset pre-loaded into memory. Setting up a single problem’s environment can be more involved than SWE-bench’s entire evaluation pipeline.</p> <p>The RL community learned how fragile this can be when MuJoCo and Gym version upgrades silently invalidated years of published baselines.</p> <p>Our answer is to <strong>pin everything and invalidate aggressively</strong>. Each problem’s <code class="language-plaintext highlighter-rouge">config.yaml</code> locks down the hardware spec, CUDA version, and runtime environment. The execution environment itself is pinned via Docker images and AMIs, so every run sees the exact same OS, drivers, and libraries. We also encourage problem contributors to design evaluators with deterministic scoring: given the same solution and the same environment, the score should be reproducible. Every evaluation result is then tagged with hashes of both the solution and the entire problem directory (including the evaluator code and config). If <em>anything</em> changes, the hash changes, and all affected scores are automatically invalidated and re-run.</p> <p>We don’t try to make scores portable across environments. Instead, we guarantee that <strong>every valid score on the leaderboard was produced under the same pinned conditions</strong>. Determinism through immutability, not through cross-environment normalization.</p> <p><img src="/assets/img/2026-02-10-evaluation/image4.png" alt="Pin, hash, and invalidate cycle"></p> <p>Evaluation determinism is becoming a first-class concern across the benchmarking community. Projects like <a href="https://github.com/cocoabench/cocoa-agent" rel="external nofollow noopener noopener noreferrer" target="_blank">CocoaBench</a> are pushing this idea further, building frameworks specifically around reproducible evaluation environments. We think this is the right direction.</p> <h3 id="4-generation--evaluation">4. Generation ≠ Evaluation</h3> <p>Many LLM benchmarks tightly couple generation and evaluation: call the API, get the output, score it immediately. We deliberately <strong>decouple</strong> them.</p> <p>The <code class="language-plaintext highlighter-rouge">gen/</code> module calls LLMs and writes plain source files (<code class="language-plaintext highlighter-rouge">solution.py</code> or <code class="language-plaintext highlighter-rouge">solution.cpp</code>). The evaluation pipeline doesn’t know or care which model wrote the code, what prompt was used, or how many attempts it took. Each evaluation runs in an <strong>isolated container or VM</strong>: solutions cannot see each other, cannot read test cases, and cannot access the network.</p> <p>Why does this matter? A human expert can drop a hand-written solution into the same directory and it gets evaluated identically. You can compare different prompting strategies, different temperatures, or different models on exactly the same evaluation path. And because each run is sandboxed, there’s no way for a solution to game the evaluator by reading other solutions or probing the scoring harness. The scoring is <strong>model-agnostic and tamper-resistant by design</strong>.</p> <h2 id="looking-ahead-agentic-submissions">Looking Ahead: Agentic Submissions</h2> <p>The clean separation above also forces us to confront a question the community is only starting to grapple with: <strong>what happens when the submission is an agent, not a file?</strong></p> <p>Today, a submission to Frontier-CS is a source file. But the frontier is moving toward agentic workflows where the model reads the problem, writes code, runs it, inspects the output, and iterates. The generation process itself has side effects: it reads the environment, runs tests, modifies code across multiple turns. The boundary between “generating a solution” and “evaluating it” starts to blur.</p> <p><img src="/assets/img/2026-02-10-evaluation/image5.png" alt="File-based vs agentic submission pipeline"></p> <p>Our current architecture handles this by keeping the boundary sharp: no matter how the solution was produced (single-shot, chain-of-thought, 50 rounds of agentic iteration), what enters the evaluation pipeline is still a source file. The agent’s process is its own business; we only score the artifact.</p> <p>But as agents get more capable, two challenges loom.</p> <p>First, <strong>reward hacking</strong>. An agent that iterates against a scoring harness is doing what any good engineer does: profile, optimize, repeat. The risk is that as agents get stronger, they overfit harder. And not all overfitting is equal. An agent that discovers temporal locality in a cache trace and designs a policy around it has learned something real. An agent that memorizes the specific access pattern in the test set and hardcodes a lookup table has learned nothing. Both produce high scores. Only one generalizes. The stronger the agent, the greater the risk of spurious overfitting, and the more pressure on us to design evaluation data that faithfully represents the real problem, so that gaming the harness and genuinely solving it become indistinguishable.</p> <p>Second, <strong>the cost model breaks</strong>. Right now, generation (API calls) and evaluation (GPU compute) are cleanly separated. You call the API, get a file, and schedule GPU time whenever it’s convenient. With agentic submissions, the agent needs to write code, run it on a GPU, inspect the results, and iterate. The GPU cluster has to stay alive while the agent thinks and makes API calls. You’re burning two expensive resources simultaneously, and each one is idle while waiting for the other. This is fundamentally a co-scheduling problem between API latency and GPU utilization, and it doesn’t have an obvious solution yet.</p> <h2 id="what-a-batch-run-looks-like">What a Batch Run Looks Like</h2> <p>Here’s what happens when the weekly CI job runs:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontier batch research <span class="nt">--workers</span> 10 <span class="nt">--clusters</span> 10 <span class="nt">--backend</span> skypilot
</code></pre></div></div> <ol> <li> <strong>Discover pairs</strong>: scan the solutions directory, match each solution file to its problem.</li> <li> <strong>Check hashes</strong>: compare solution/problem hashes against stored results. Skip unchanged pairs; invalidate stale results.</li> <li> <strong>Group by resources</strong>: partition pairs by <code class="language-plaintext highlighter-rouge">ResourceSignature</code>. A CUDA kernel problem and a CPU algorithm problem land in different groups.</li> <li> <strong>Create cluster pools</strong>: provision SkyPilot clusters for each group, proportionally allocated.</li> <li> <strong>Dispatch</strong>: workers pull pairs from a shared queue, acquire a cluster from the matching pool, run the evaluation, return the cluster.</li> <li> <strong>Record</strong>: each result is saved with its hashes. Interrupted runs resume from exactly where they left off.</li> </ol> <p>The same pipeline runs for the algorithmic track, with a go-judge server instead of SkyPilot clusters.</p> <h2 id="what-this-enables">What This Enables</h2> <ul> <li> <strong>The leaderboard stays honest.</strong> Every score is tagged with environment hashes. Stale results are caught automatically. Failures are tracked via status metadata, not silently recorded as score = 0.</li> <li> <strong>Contributors can validate locally.</strong> <code class="language-plaintext highlighter-rouge">frontier eval</code> runs the same scoring logic on your laptop (via Docker) that the CI runs in the cloud.</li> <li> <strong>New models get evaluated fast.</strong> Hash-based resume means only new or changed pairs are evaluated. Right-sized cluster pools mean no wasted compute.</li> </ul> <p><img src="/assets/img/2026-02-10-evaluation/image6.png" alt="Batch run terminal output (mockup)"></p> <h2 id="try-it-yourself">Try It Yourself</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/FrontierCS/Frontier-CS.git
<span class="nb">cd </span>Frontier-CS <span class="o">&amp;&amp;</span> uv <span class="nb">sync</span>

<span class="c"># Evaluate a single problem locally</span>
frontier <span class="nb">eval </span>research flash_attn solution.py <span class="nt">--backend</span> docker

<span class="c"># List all available problems</span>
frontier list
</code></pre></div></div> <p>For the full architecture details, see <a href="https://github.com/FrontierCS/Frontier-CS/blob/main/ARCHITECTURE.md" rel="external nofollow noopener noopener noreferrer" target="_blank">ARCHITECTURE.md</a> in the repo.</p> <h2 id="getting-in-touch">Getting in Touch</h2> <p>We are always looking for more problems to add for our next release and evaluating on <a href="https://github.com/FrontierCS/Frontier-CS/blob/main/SUBMIT.md" rel="external nofollow noopener noopener noreferrer" target="_blank">more models and agents</a>. We love to hear about your comments and feedback! Join us on <a href="https://discord.com/invite/k4hd2nU4UE" rel="external nofollow noopener noopener noreferrer" target="_blank">discord</a> or <a href="mailto:frontiercs@berkeley.edu">email us</a>!</p> <p>And if you find Frontier-CS useful, please consider <a href="https://github.com/FrontierCS/Frontier-CS" rel="external nofollow noopener noopener noreferrer" target="_blank">giving us a star on GitHub</a>!</p> </d-article> </div> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>